# Appliquez des filtres sur vos vid√©os avec Javascript

D√©nicher des images exemptes de retouches rel√®ve presque de l'exploit. Et pour cause, lorsque nous ne les manipulons pas nous m√™mes, nos appareils photo, t√©l√©phonnes, et autres logiciels traitent automatiquement nos images √† notre insu, que ce soit pour am√©liorer la nettet√©, r√©hausser la luminosit√©, am√©liorer la r√©partition des couleurs, etc.

S'il convient d'effectuer tout ces traitements en amont pour des raisons d'optimisations, il est parfois n√©cessaire d'appliquer des filtres sur un contenu vid√©o √† la vol√©e. Dans ce tutoriel ce tutoriel, nous impl√©menterons une solution permettant de manipuler le flux de notre image directement depuis notre navigateur.

Nous impl√©menterons des filtres basiques pour illustrer nos exemples, mais le traitement d'image restera relativement basique.

Maintenant que vous savez √† quoi vous attendre, inutile d'attendre plus longtemps : commen√ßons √† coder !

## Principe

La m√©thode pr√©sent√©e dans ce tutoriel consiste √† :
- Int√©grer une vid√©o la notre page, gr√¢ce √† la balise `<video>` (id√©alement en cach√©)
- R√©cup√©rer son flux vid√©o dans un objet `ImageData`
- Effectuer le traitement de l'image dessus
- Afficher le r√©sultat dans une balise `<canvas>`.


### Mise en oeuvre


#### Int√©grer une vid√©o √† la page

Rien de bien complexe si vous connaissez un peu le HTML.
```html
 <video
   id="tuto-video"
   src="your-video-url"
   width="300"
   height="300"
   controls
></video>
```


#### R√©cup√©ration du flux vid√©o

Si vous vous √™tes d√©j√† int√©ress√©s √† l'encodage des vid√©os, vous savez qu'obtenir le flux de pixels √† partir d'un fichier est une t√¢che relativement complexe. Une bonne connaissance des formats vid√©os est indispensable.
Toutefois, gr√¢ce √† la balise `<video>`, nous somme d√©livr√© de cette contrainte. Le navigateur s'occupe d'extraire les pixels, d'effectuer le rendu, et en charge la plupart des formats habituels.

Il ne nous reste qu'√† r√©cup√©rer ce flot de pixel dans notre objet `ImageData`.  Encore une fois, le navigateur va nous rendre un immense service. En effet, l'extraction s'op√®re en quelques lignes seulement en utilisant un objet `canvas`.

```js
  const video = document.getElementById('tuto-video');

// Create canvas for video's pixel extraction
const extractPixelCanvas = document.createElement('canvas');
const extractPixelContext = extractPixelCanvas.getContext('2d');

/**
 * @return {ImageData}
 */
function extractVideoImageData(video, width, height) {
    // avoid unnecessary resize as much as possible (optimization)
    if (extractPixelCanvas.width !==  width) {
        extractPixelCanvas.width =  width;
    }

    if (extractPixelCanvas.height !==  height) {
        extractPixelCanvas.height = height;
    }
    
    extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
}
```


#### Manipuler l'objet `ImageData`

Le choix de cette classe est motiv√© par la facilit√© d'afficher le contenu d'une instance d'`ImageData` dans un canvas : 
```js
canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

La structure de l'objet lui m√™me est relativement simple ; une instance poss√®de les propri√©t√©s **width** et **height** correspondant √† la r√©solution de l'image, ainsi que **data**, un tableau 8 bits non sign√©s (ou `Uint8ClampedArray`).
Ce dernier attribut nous int√©resse plus particuli√®rement car il contient les pixels de notre image. Chaque pixel est encod√© en RGBA, avec un alpha compris entre 0 et 255.

Pour modifier une image √† la vol√©e, il suffit de modifier les pixels contenu dans **data**.

Un exemple tir√© de [la documentation](https://developer.mozilla.org/en-US/docs/Web/API/ImageData/ImageData).

```js
// Iterate through every pixel
for (let i = 0; i < arr.length; i += 4) {
  arr[i + 0] = 0;    // R value
  arr[i + 1] = 190;  // G value
  arr[i + 2] = 0;    // B value
  arr[i + 3] = 255;  // A value
}
```

Cette classe sera au coeur de l'impl√©mentation de nos filtre, vous trouverez donc quelques exemples suppl√©mentaires par la suite üòâ


#### Afficher le r√©sultat dans un `<canvas>`

Rien de bien complexe si vous avez compris les paragraphes pr√©c√©dents.

```html
<!--html-->
<canvas id="tuto-canvas"></canvas>
```
```js
//js
const canvas = document.getElementById('tuto-canvas');
canvasContext2D = canvas.getContext('2d');

const instanceOfImageData = applyYourAmazingFilter(/* ... */);

canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

## Un filtre, oui ; mais aussi une animation !

L'utilisation d'un filtre sur un flux vid√©o est consid√©r√©e (ici) comme une animation. L'application du filtre √† la vol√©e sur des images constitue notre **m√©thode de rendu**, tandis que la synchronisation entre le canvas et le lecteur vid√©o d√©terminera le comportement de notre **boucle de rendu**. Cette remarque vous para√Æt floue ? Peut-√™tre devriez-vous jeter un coup d'≈ìil l'article [Fa√Ætes vos propres animations en JS](https://dev.to/qphilippot/faites-vos-propres-animations-en-js-34ok).


### Synchroniser l'animation avec le lecteur vid√©o - D√©finir la boucle de rendu

Nous souhaitons que notre boucle de rendu soit pilot√© par le lecteur vid√©o. C'est √† dire que l'animation se lance quand on clique sur play, qu'elle s'arr√™te lorsqu'on atteint la fin de la vid√©o ou que l'on met pause (pour ne pas rafra√Æchir une image qui ne change pas, ce serait dommage de g√¢cher des ressources CPU pour rien).

Pour rappel, la **boucle de rendu** s'occupera de rafra√Æchir automatiquement notre canvas.

```js
const animation = new Animation({ /* ‚Ä¶ */ });

video.addEventListener('play', () => {
   animation.play();
});

video.addEventListener('pause', () => {
   animation.pause();
});

video.addEventListener('end', () => {
   animation.pause();
});

// render animation once when we click on timeline
video.addEventListener('timeupdate', () => {
   animation.askRendering()
});
```

### Impl√©mentation d'un filtre - D√©finir une m√©thode de rendu

Une fois que nous savons comment extraire les pixels de notre vid√©o, et que nous avons d√©finit notre boucle de rendu afin que notre canvas se rafra√Æchisse automatiquement, il ne reste plus qu'√† d√©finir notre m√©thode de rendu.

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    // rendering method is here
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        // apply filter over imageData here;
        animation.clear();
        context.putImageData(imageData, 0, 0);
        }
    }
);

```



Utilisation classique de canvas et d'image data. Il existe beaucoup de documentation √† ce sujet, je vous redirige vers la documentation pour plus d'explication : https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Pixel_manipulation_with_canvas

Note : ce tutoriel montre comment appliquer des filtres sur des images. Il constitue un excellent compl√©ment √† ce tuto. N'h√©sitez pas √† y jeter un oeil !


## R√©sum√© 

```js
import Animation from '../../shared/animation.model';

document.addEventListener('DOMContentLoaded', () => {
    // Create canvas for video's pixel extraction
    const extractPixelCanvas = document.createElement('canvas');
    const extractPixelContext = extractPixelCanvas.getContext('2d');

    function extractVideoImageData(video, width, height) {
        // avoid unnecessary resize as much as possible (optimization)
        if (extractPixelCanvas.width !==  width) {
            extractPixelCanvas.width =  width;
        }
    
        if (extractPixelCanvas.height !==  height) {
           extractPixelCanvas.height = height;
        }


       extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
       return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    }
    
    const video = document.getElementById('tuto-video');


    const animation = new Animation({
        canvas: document.getElementById('tuto-canvas'),
        render: (context, canvas) => {
            const imageData = extractVideoImageData(video, canvas.width, canvas.height);
            
            // apply filter over imageData here;
            
           animation.clear();
           context.putImageData(imageData, 0, 0);
       }
   });


    video.addEventListener('play', () => {
        animation.play();
    });

    video.addEventListener('pause', () => {
        animation.pause();
    });

    video.addEventListener('end', () => {
        animation.pause();
    });

    video.addEventListener('timeupdate', () => {
        animation.askRendering()
    })
});
```

## R√©sultat Pr√©liminaire


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/no-filter-1.gif?raw=true)

*¬´ - Hein ? Je ne vois aucune diff√©rence‚Ä¶¬ª* üôà


Pr√©cis√©ment ! Nous n'avons encore appliqu√© aucun filtre. Toutefois, on constate que notre flux vid√©o est bel et bien r√©pliqu√© sans d√©formation ni latence.

Pour appliquer un filtre sur l'image, il suffira d'appliquer un traitement sur l'image data extraire dans la m√©thode de rendu. Le tutoriel pourrait s'arr√™ter l√†, le m√©canisme lui n'√©voluera pas.

Mais je ne peux pas me r√©signer √† m'arr√™ter alors que cela commen√ßait tout juste √† devenir cool ! 

# Exemple d'impl√©mentation de filtres en JS

## Grayscale

Classique, s'il en est. Nous allons simplement transformer les pixels RGB en niveau de gris.

```js
// get grayscale value for a pixel in buffer

function rgbToGrayscale(buffer, offset) {
   return Math.ceil((
       0.30 * buffer[offset] +
       0.59 * buffer[offset + 1] +
       0.11 * buffer[offset + 2]
   ) * (buffer[offset + 4] / 255.0));
}

/**
* @param {Uint8Array} pixelBuffer
*/
function applyGrayscaleFilter(pixelBuffer) {
   for (let offset = 0; offset <pixelBuffer.length; offset += 4) {
       const grayscale = rgbToGrayscale(pixelBuffer, offset);
       pixelBuffer[offset] = grayscale;
       pixelBuffer[offset + 1] = grayscale;
       pixelBuffer[offset + 2] = grayscale;
       pixelBuffer[offset + 3] = 255;
   }
}


const animation = new Animation({
   canvas: document.getElementById('tuto-canvas'),
   render: (context, canvas) => {
       const imageData = extractVideoImageData(video, canvas.width, canvas.height);
       applyGrayscaleFilter(imageData.data);
       
       animation.clear();
       context.putImageData(imageData, 0, 0);
   }
});
```


Pour chaque pixel, nous rempla√ßons les canaux RGB par leur niveau de gris.

Intuitivement, il serait tentant d'utiliser une moyenne des composantes `RGB`. Toutefois, puisque notre oeil ne per√ßoit pas toutes les couleurs avec la m√™me sensibilit√©, une telle m√©thode ne serait pas optimale pour l'≈ìil humain. On utilise √† la place la **luminance** du pixel. Ce n'est rien d'autre qu'une combinaison des trois couleurs, mais avec des coefficients pour prendre en compte notre sensibilit√© √† la perception des couleurs.

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale.gif?raw=true)

## Supporter les interactions souris

Une animation c'est bien, mais une animation qui interagit avec la souris, c'est mieux ! Nous allons transformer notre code afin d'appliquer le filtre seulement si notre pointeur se trouve √† l'int√©rieur du canvas.

Nous pouvons d'ores et d√©j√† transformer notre m√©thode de rendu de la sorte :

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        
        // compute isPointerHoverCanvas ...

        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
        
        animation.clear();
        context.putImageData(imageData, 0, 0);
    }
});
```

Plusieurs solutions existent pour d√©terminer si le curseur est, ou n'est pas, √† l'int√©rieur de notre canvas. Selon l'approche, certaines sont plus appropri√©es que d'autres.

### D√©terminer la position de la souris par rapport au canvas

La solution la plus simple consiste √† r√©cup√©rer les coordonn√©es de la souris, les coordonn√©es du canvas, et de v√©rifier si le curseur de la souris est dans la bo√Æte englobante du canvas (ou hitbox).

```js
const pointerCoords = {x: 0, y: 0};

document.addEventListener('pointermove', event => {
    pointerCoords.x = event.clientX;
    pointerCoords.y = event.clientY;
});

const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        // ‚Ä¶

        const boundingBox = canvas.getBoundingClientRect();


        const isPointerHoverCanvas = (
            pointerCoords.x >= boundingBox.left &&
            pointerCoords.y >= boundingBox.top &&
            pointerCoords.x < boundingBox.right &&
            pointerCoords.y < boundingBox.bottom
        );


        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
    }
});
```

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-1.gif?raw=true)

## On corse le jeu !

On va appliquer le filtre grayscale sur toute l'image, et ne faire appara√Ætre les couleurs que sur les pixels autour de notre curseur.

Petite subtilit√© : pour cr√©er un effet plus lisse, on d√©terminera un cercle √† l'int√©rieur duquel les pixels seront color√©s, mais avec une intensit√© inversement proportionnelle √† la distance au centre‚Ä¶

### Rappel G√©om√©trique
Un cercle peut √™tre d√©fini par son centre, et un rayon. Dans notre cas, le centre du cercle correspond tout simplement aux coordonn√©es du curseur. Quand au rayon, nous prendrons une valeur arbitraire.

D√©terminer si un point est dans un cercle revient √† calculer la **collision** entre un point et un cercle.

De nombreux tutoriels existent et expliquent les formules de collision. Pour en savoir plus sur les m√©thodes de collision, jetez un oeil √† ce tutoriel : http://www.jeffreythompson.org/collision-detection/point-circle.php

### Approche g√©n√©rale

Nous allons modifier la m√©thode de rendu afin de v√©rifier, pour chaque pixel de notre imageData s'il est, ou n'est pas, dans le cercle centr√© sur la position de notre curseur. Afin de faciliter le calcul, nous allons nous placer dans le rep√®re g√©om√©trique de notre canvas.

```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    
    const coordsRelativeToCanvas = PointerCoordsHelper.getCoordsRelativeToElement(
        canvas,
        pointerCoords.x,
        pointerCoords.y
    );

    const buffer = imageData.data;
    
    // apply to the whole buffer, execept a circle defined by pointer position
    for (let offset = 0; offset < buffer.length; offset += 4) {
        const pixelOffset = (offset / 4); // pixels have 4 channel in ImageData
        const pixelX = pixelOffset % canvas.width;
        const pixelY = pixelOffset / canvas.width;

        // arbitrary radius
        const radius = 50;

        const isInCircle = CollisionHelper.isPointInCircle(
            pixelX, pixelY,
            coordsRelativeToCanvas.x, coordsRelativeToCanvas.y,
            radius
        );

        const grayscale = rgbToGrayscale(buffer, offset);

        if (isInCircle === false) {
            buffer[offset] = grayscale;
            buffer[offset + 1] = grayscale;
            buffer[offset + 2] = grayscale;
            buffer[offset + 3] = 255;
        } else {
            const distance = GeometryHelper.getDistanceBetween2DPoints(
                pixelX, pixelY,
                coordsRelativeToCanvas.x, coordsRelativeToCanvas.y
            );

            const weight = distance / radius;
            // apply a weight in order to let color intensity increase from the outside to the center
            buffer[offset] = weight * grayscale + (1 - weight) * buffer[offset];
            buffer[offset + 1] = weight * grayscale + (1 - weight) * buffer[offset + 1];
            buffer[offset + 2] = weight * grayscale + (1 - weight) * buffer[offset + 2];
            buffer[offset + 3] = 255;
        }
    }


    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-2.gif?raw=true)

<details>
<summary>‚ö†Ô∏è Remarque sur le calcul de coordonn√©es ( Avanc√©) ‚ö†Ô∏è</summary>

Le lecteur attentif aura remarqu√© qu'on calcule la position du curseur relativement √† notre canvas (c'est-√†-dire telle que l'origine soit le coin sup√©rieur gauche du canvas). 

Ce n'est pas obligatoire, on aurait pu impl√©menter le m√™me filtre en utilisant directement les coordonn√©es du pointeur dans la fen√™tre, mais les √©quations auraient √©t√©s plus compliqu√©es. 

De plus, l'on est parfois amen√© √† travailler avec des canvas dont la r√©solution (*pixel th√©orique*) est diff√©rente de sa taille (*pixel physique*). Dans ce cas, vu que notre algorithme it√®re sur les pixels th√©oriques du canvas `animation.context.width` ou `animation.canvas.width`. Il faudra modifier les √©quations pour prendre en compte ce changement de rep√®re suppl√©mentaire‚Ä¶

</details>

## Sa vision est bas√©e sur le mouvement !


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-1.gif?raw=true)

"Mouais, c'est pas terrible..."

En effet, on peut mieux faire ! N√©anmoins, concentrons nous sur le positif, nous avons un d√©but de quelque chose ! Nous parvenons √† d√©celer les contours du perroquet lorsque celui-ci entre en mouvement. Le probl√®me c'est que ses d√©placements sont lents, et pas forc√©ment perceptibles d'une frame sur l'autre. De plus, si les mouvements entre deux slices sont minimes, et que nous rafraichissons notre en 60 fps (ce que tentera de faire naturellement notre classe animation), soit une image approximativement toutes les 16ms, et sachant que la persistance r√©tinienne est de l'ordre de 1/25 de secondes (40 ms), il y a de grande chance pour que le mouvement, bien que visible dans notre canvas, ne nous soit pas perceptible.

### Alternative simple

Plut√¥t que de se lancer dans un calcul p√©rilleux sur le taux de rafra√Æchissement optimal, nous allons opter pour une solution b√™te et m√©chante : calculer le mouvement en prenant en compte, non pas la derni√®re frame, mais les N derni√®res frames.

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-2.gif?raw=true)


Voici une tentative se basant sur les 5 derni√®res frames, avec coefficient √©quivalent √† 15. Si les mouvements sont davantage perceptibles, le co√ªt en calcul lui est nettement plus √©lev√©. On passe de 60 fps √† un peu plus de 20 fps. Rien de plus normal, on a presque tripl√© la charge de travail.

Il existe des m√©thodes permettant d'obtenir un r√©sultat plus propre, et moins gourmant en calcul, mais √©galement moins simple √† expliquer. Et vu que ce n'est pas le but premier de cet article, je les passe sous silence. Sachez juste qu'en utilisant des images s√©par√©es dans le temps (r√©gulier, par exemple 40 ms) et une simple moyenne au lieu de frame s√©par√©es par des it√©rations de notre boucle de rendu (potentiellement irr√©guli√®re est trop courte), on se rapproche d'un comportement naturel ;)

## La vie en bleu 

Dans ce dernier exemple, je propose de teindre ce cher perroquet en bleu. Pour atteindre notre objectif, consid√©rons la couleur de son plumage d'origine. Il n'est pas simplement rouge, mais couvre une nuance de rouge. Le filtre devra prendre en compte toutes ces nuances, notamment pour proposer un rendu r√©aliste prenant en compte la pigmentation naturelle des plumes ainsi les variations de luminosit√©.

### Rappel sur la repr√©sentation de la couleur
La repr√©sentation des couleurs dans les `ImageData` est en `RGBA`. En d'autre terme, la couleur finale est obtenue a partir d'un m√©lange des quatre composantes.

Une solution na√Øve consisterait √† supprimer la dimension rouge (mettre toutes les intensit√©s √† 0). L'inconveignant de cette approche est que dans l'espace `RGBA`, toutes les couleurs ont une part contiennent une part de rouge. Autrement dit, si l'on modifie la composante `R`, quasiment toutes les couleurs seront impact√©es.

Le probl√®me ici est que l'on se restreint √† l'utilisation d'un seul espace couleur (`RGBA`), dans lequels toutes nos couleurs sont fortement corr√©l√©es √† la couleur rouge. Toutefois, il en existe d'autres espaces, lesquels permettent d'exprimer des couleurs selon d'autres dimensions, et qui proposent des √©quations pour passer d'un espace couleur vers un autre. Selon le cas d'usage, certains espaces couleurs sont plus pratiques que d'autres (`YCrCb` pour la compression, `CMJN` pour l'impression, etc).

Dans le cas pr√©sent, l'ensemble `HSL` *Hue Saturation Lightness*, ou `TSV` en fran√ßais semble le plus appropri√©. Dans cet espace, la *teinte* des couleurs est d√©finie via un cercle colorim√©trique. Pour transformer du "rouge" en "bleu", il suffit de d√©terminer une section du cercle que l'on souhaite remplacer, et d'y coller la section par laquelle on souhaite le remplacer.

![Alt Text](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/HSV_cone.png/300px-HSV_cone.png)

### Principe du filtre

* R√©cup√©rer la couleur du nos pixels `RGBA`
* Les convertir en `HSL`
* Manipuler les teintes *rouges* et les remplacer par des *bleues*
* Reconvertir en `RGBA`
* Remplir l'instance `ImageData` avec ces pixels modifi√©s

### Impl√©mentation

Concernant les fonctions de transformations `HSL` vers `RGBA` et r√©ciproquement, je vous laisse checker le git. Pour en savoir plus : [https://en.wikipedia.org/wiki/HSL_and_HSV](https://en.wikipedia.org/wiki/HSL_and_HSV).

Pour des raisons de performances, nous allons impl√©menter une Look Up Table (LUT), c'est √† dire une table de correspondance pour toutes nos couleurs. L'enjeu est de ne pas calculer toutes les correspondances de couleurs √† la vol√©e, pixels par pixels, √† chaque nouvelle image, mais de les calculer une bonne fois pour toute au lancement de la page. 

La m√©thode de rendu n'aura qu'√† lire dans cette LUT pour y lire les r√©sultats et gagner un temps pr√©cieux (et un meilleur frame rate).
#### Calcul de la LUT
```js
function generateRedToBlueLUT() {
    const size = 16777216; // 256 * 256 * 256
    const lut = new Array(size);

    // initialize all colors to black
    for (let i = 0; i < size; i++) {
        lut[i] = [0, 0, 0];
    }

    // iterate through RGB combinaisons
    for (let redOffset = 0; redOffset < 256; redOffset++) {
        for (let greenOffset = 0; greenOffset < 256; greenOffset++) {
            for (let blueOffset = 0; blueOffset < 256; blueOffset++) {
                // Use a pool design pattern
                // If you want to implements it without object pool, juste replace it by [0, 0, 0]
                const rgb = vec3Pool.getOne();
                const hsl = vec3Pool.getOne();
                
                rgb[0] = redOffset;
                rgb[1] = greenOffset;
                rgb[2] = blueOffset;

                // color conversion, check sources for detailled implementation 
                rgbToHSL(rgb, hsl);

                // Clamp saturation and lightness
                hsl[1] = Math.max(0, Math.min(hsl[1], 1));
                hsl[2] = Math.max(0, Math.min(hsl[2], 1));

                // Here is the trick: hue is represented by a degree angle
                // We want : 0 <= hue < 360
                if (hsl[0] < 0) {
                    hsl[0] += 360;
                }

                hsl[0] = hsl[0] % 360;

                // Assume that :
                // - "red" hues are between 340¬∞ and 20¬∞
                // - "blue" are between 140¬∞ and 220¬∞
                
                // replace hue
                if (hsl[0] > 340 && hsl[2] < 0.85) {
                    hsl[0] -= 120;
                }

                else if (hsl[0] < 20 && hsl[2] < 0.85) {
                    hsl[0] += 240;
                }

                // sanitize angle : 0 <= hue < 360 
                if (hsl[0] < 0) {
                    hsl[0] += 360;
                }

                hsl[0] = hsl[0] % 360;
                
                hslToRGB(hsl, rgb);

                // store RGBA converted into lut
                lut[redOffset * 65536 + greenOffset * 256 + blueOffset] = Array.from(rgb);
                
                // recycle instance, only for object pool implementation
                vec3Pool.recycle(rgb);
                vec3Pool.recycle(hsl);
            }
        }
    }

    return lut;
}
window.lut = generateRedToBlueLUT();
 ```
Plusieurs remarques sur cette impl√©mentation :
* Notre LUT est un tableau. On calcule l'index de chaque couleur par la formule `R * 255 * 255 + G * 255 + B`
* Pour des raisons de performances, on utilise un object pool design pattern. En effet, on va instancier pas mal de petits tableaux pour calculer notre LUT, et cela peut surcharger inutilement la m√©moire du navigateur. Pour en savoir plus sur l'impl√©mentation de l'object pool design pattern en JS, lisez l'article suivant : [Optimisez vos applications JS avec l'Object Pool Design Pattern !](https://dev.to/qphilippot/optimisez-vos-applications-js-avec-l-object-pool-design-pattern-3g8)
* Les calculs d'angles sont empiriques, √† partir du cercle colorim√©trique. D'ailleurs, en regardant attentivement le rendu, on peut s'apercevoir que la "teinture" n'est pas parfaite, et que quelques pointens de rouges se prom√®nent √ßa et l√† ;)

#### Coup d'oeil sur la m√©thode de rendu

```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    const buffer = imageData.data;

    for (let offset = 0; offset < buffer.length; offset += 4) {
        const r = buffer[offset];
        const g = buffer[offset + 1];
        const b = buffer[offset + 2];

        // 65536 = 256 * 256
        const lutIndex = r * 65536 + g * 256 + b;
        
        // just replace color by pre-computed value
        const color = window.lut[lutIndex];

        buffer[offset] = color[0];
        buffer[offset + 1] = color[1];
        buffer[offset + 2] = color[2];
        buffer[offset + 3] = 255;

    }
    
    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```

Et voici un beau perroquet haut en couleur ! :D

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/blue-1.gif?raw=true)

## Conclusion

J'esp√®re sinc√®rement que ce tutoriel vous a plus. Le principe derri√®re l'utilisation de filtres en live est assez simple √† impl√©menter, mais n√©cessitait bien selon moi quelques exemples pour comprendre son utilisation. Je suis pass√© assez rapidement sur certains points pour essayer de ne pas trop d√©vier de l'objectif principal : torturer ce pauvre oiseau utiliser une boucle de rendu pour appliquer des filtres en temps r√©els.

N'h√©sitez pas √† me faire part de vos commentaires ou de vos remarques, c'est toujours un plaisir ;)