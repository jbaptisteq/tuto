# Appliquez des filtres sur vos vid√©os avec Javascript

D√©nicher des images exemptes de retouches rel√®ve presque de l'exploit. Et pour cause, lorsque nous ne les manipulons pas nous m√™mes, nos appareils photo, t√©l√©phonnes, et autres logiciels traitent automatiquement nos images √† notre insu, que ce soit pour am√©liorer la nettet√©, r√©hausser la luminosit√©, am√©liorer la r√©partition des couleurs, etc.

S'il convient d'effectuer tout ces traitements en amont pour des raisons d'optimisations, il est parfois n√©cessaire d'appliquer des filtres sur un contenu vid√©o √† la vol√©e. Dans ce tutoriel ce tutoriel, nous impl√©menterons une solution permettant de manipuler le flux de notre image directement depuis notre navigateur.

Nous impl√©menterons des filtres basiques pour illustrer nos exemples, mais le traitement d'image restera relativement basique.

Maintenant que vous savez √† quoi vous attendre, inutile d'attendre plus longtemps : commen√ßons √† coder !

## Principe

La m√©thode pr√©sent√©e dans ce tutoriel consiste √† :
- Int√©grer une vid√©o la notre page, gr√¢ce √† la balise `<video>` (id√©alement en cach√©)
- R√©cup√©rer son flux vid√©o dans un objet `ImageData`
- Effectuer le traitement de l'image dessus
- Afficher le r√©sultat dans une balise `<canvas>`.


### Mise en oeuvre


#### Int√©grer une vid√©o √† la page

Rien de bien complexe si vous connaissez un peu le HTML.
```html
 <video
   id="tuto-video"
   src="your-video-url"
   width="300"
   height="300"
   controls
></video>
```


#### R√©cup√©ration du flux vid√©o

Si vous vous √™tes d√©j√† int√©ress√©s √† l'encodage des vid√©os, vous savez qu'obtenir le flux de pixels √† partir d'un fichier est une t√¢che relativement complexe. Une bonne connaissance des formats vid√©os est indispensable.
Toutefois, gr√¢ce √† la balise `<video>`, nous somme d√©livr√© de cette contrainte. Le navigateur s'occupe d'extraire les pixels, d'effectuer le rendu, et en charge la plupart des formats habituels.

Il ne nous reste qu'√† r√©cup√©rer ce flot de pixel dans notre objet `ImageData`.  Encore une fois, le navigateur va nous rendre un immense service. En effet, l'extraction s'op√®re en quelques lignes seulement en utilisant un objet `canvas`.

```js
  const video = document.getElementById('tuto-video');

// Create canvas for video's pixel extraction
const extractPixelCanvas = document.createElement('canvas');
const extractPixelContext = extractPixelCanvas.getContext('2d');

/**
 * @return {ImageData}
 */
function extractVideoImageData(video, width, height) {
    // avoid unnecessary resize as much as possible (optimization)
    if (extractPixelCanvas.width !==  width) {
        extractPixelCanvas.width =  width;
    }

    if (extractPixelCanvas.height !==  height) {
        extractPixelCanvas.height = height;
    }
    
    extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
}
```


#### Manipuler l'objet `ImageData`

Le choix de cette classe est motiv√© par la facilit√© d'afficher le contenu d'une instance d'`ImageData` dans un canvas : 
```js
canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

La structure de l'objet lui m√™me est relativement simple ; une instance poss√®de les propri√©t√©s **width** et **height** correspondant √† la r√©solution de l'image, ainsi que **data**, un tableau 8 bits non sign√©s (ou `Uint8ClampedArray`).
Ce dernier attribut nous int√©resse plus particuli√®rement car il contient les pixels de notre image. Chaque pixel est encod√© en RGBA, avec un alpha compris entre 0 et 255.

Pour modifier une image √† la vol√©e, il suffit de modifier les pixels contenu dans **data**.

Un exemple tir√© de [la documentation](https://developer.mozilla.org/en-US/docs/Web/API/ImageData/ImageData).

```js
// Iterate through every pixel
for (let i = 0; i < arr.length; i += 4) {
  arr[i + 0] = 0;    // R value
  arr[i + 1] = 190;  // G value
  arr[i + 2] = 0;    // B value
  arr[i + 3] = 255;  // A value
}
```

Cette classe sera au coeur de l'impl√©mentation de nos filtre, vous trouverez donc quelques exemples suppl√©mentaires par la suite üòâ


#### Afficher le r√©sultat dans un `<canvas>`

Rien de bien complexe si vous avez compris les paragraphes pr√©c√©dents.

```html
<!--html-->
<canvas id="tuto-canvas"></canvas>
```
```js
//js
const canvas = document.getElementById('tuto-canvas');
canvasContext2D = canvas.getContext('2d');

const instanceOfImageData = applyYourAmazingFilter(/* ... */);

canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

## Un filtre, oui ; mais aussi une animation !

L'utilisation d'un filtre sur un flux vid√©o est consid√©r√©e (ici) comme une animation. L'application du filtre √† la vol√©e sur des images constitue notre **m√©thode de rendu**, tandis que la synchronisation entre le canvas et le lecteur vid√©o d√©terminera le comportement de notre **boucle de rendu**. Cette remarque vous para√Æt floue ? Peut-√™tre devriez-vous jeter un coup d'≈ìil l'article [Fa√Ætes vos propres animations en JS](https://dev.to/qphilippot/faites-vos-propres-animations-en-js-34ok).


### Synchroniser l'animation avec le lecteur vid√©o - D√©finir la boucle de rendu

Nous souhaitons que notre boucle de rendu soit pilot√© par le lecteur vid√©o. C'est √† dire que l'animation se lance quand on clique sur play, qu'elle s'arr√™te lorsqu'on atteint la fin de la vid√©o ou que l'on met pause (pour ne pas rafra√Æchir une image qui ne change pas, ce serait dommage de g√¢cher des ressources CPU pour rien).

Pour rappel, la **boucle de rendu** s'occupera de rafra√Æchir automatiquement notre canvas.

```js
const animation = new Animation({ /* ‚Ä¶ */ });

video.addEventListener('play', () => {
   animation.play();
});

video.addEventListener('pause', () => {
   animation.pause();
});

video.addEventListener('end', () => {
   animation.pause();
});

// render animation once when we click on timeline
video.addEventListener('timeupdate', () => {
   animation.askRendering()
});
```

### Impl√©mentation d'un filtre - D√©finir une m√©thode de rendu

Une fois que nous savons comment extraire les pixels de notre vid√©o, et que nous avons d√©finit notre boucle de rendu afin que notre canvas se rafra√Æchisse automatiquement, il ne reste plus qu'√† d√©finir notre m√©thode de rendu.

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    // rendering method is here
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        // apply filter over imageData here;
        animation.clear();
        context.putImageData(imageData, 0, 0);
        }
    }
);

```



Utilisation classique de canvas et d'image data. Il existe beaucoup de documentation √† ce sujet, je vous redirige vers la documentation pour plus d'explication : https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Pixel_manipulation_with_canvas

Note : ce tutoriel montre comment appliquer des filtres sur des images. Il constitue un excellent compl√©ment √† ce tuto. N'h√©sitez pas √† y jeter un oeil !


## R√©sum√© 

```js
import Animation from '../../shared/animation.model';

document.addEventListener('DOMContentLoaded', () => {
    // Create canvas for video's pixel extraction
    const extractPixelCanvas = document.createElement('canvas');
    const extractPixelContext = extractPixelCanvas.getContext('2d');

    function extractVideoImageData(video, width, height) {
        // avoid unnecessary resize as much as possible (optimization)
        if (extractPixelCanvas.width !==  width) {
            extractPixelCanvas.width =  width;
        }
    
        if (extractPixelCanvas.height !==  height) {
           extractPixelCanvas.height = height;
        }


       extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
       return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    }
    
    const video = document.getElementById('tuto-video');


    const animation = new Animation({
        canvas: document.getElementById('tuto-canvas'),
        render: (context, canvas) => {
            const imageData = extractVideoImageData(video, canvas.width, canvas.height);
            
            // apply filter over imageData here;
            
           animation.clear();
           context.putImageData(imageData, 0, 0);
       }
   });


    video.addEventListener('play', () => {
        animation.play();
    });

    video.addEventListener('pause', () => {
        animation.pause();
    });

    video.addEventListener('end', () => {
        animation.pause();
    });

    video.addEventListener('timeupdate', () => {
        animation.askRendering()
    })
});
```

## R√©sultat Pr√©liminaire


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/no-filter-1.gif?raw=true)

*¬´ - Hein ? Je ne vois aucune diff√©rence‚Ä¶¬ª* üôà


Pr√©cis√©ment ! Nous n'avons encore appliqu√© aucun filtre. Toutefois, on constate que notre flux vid√©o est bel et bien r√©pliqu√© sans d√©formation ni latence.

Pour appliquer un filtre sur l'image, il suffira d'appliquer un traitement sur l'image data extraire dans la m√©thode de rendu. Le tutoriel pourrait s'arr√™ter l√†, le m√©canisme lui n'√©voluera pas.

Mais je ne peux pas me r√©signer √† m'arr√™ter alors que cela commen√ßait tout juste √† devenir cool ! 

# Exemple d'impl√©mentation de filtres en JS

## Grayscale

Classique, s'il en est. Nous allons simplement transformer les pixels RGB en niveau de gris.

```js
// get grayscale value for a pixel in buffer

function rgbToGrayscale(buffer, offset) {
   return Math.ceil((
       0.30 * buffer[offset] +
       0.59 * buffer[offset + 1] +
       0.11 * buffer[offset + 2]
   ) * (buffer[offset + 4] / 255.0));
}

/**
* @param {Uint8Array} pixelBuffer
*/
function applyGrayscaleFilter(pixelBuffer) {
   for (let offset = 0; offset <pixelBuffer.length; offset += 4) {
       const grayscale = rgbToGrayscale(pixelBuffer, offset);
       pixelBuffer[offset] = grayscale;
       pixelBuffer[offset + 1] = grayscale;
       pixelBuffer[offset + 2] = grayscale;
       pixelBuffer[offset + 3] = 255;
   }
}


const animation = new Animation({
   canvas: document.getElementById('tuto-canvas'),
   render: (context, canvas) => {
       const imageData = extractVideoImageData(video, canvas.width, canvas.height);
       applyGrayscaleFilter(imageData.data);
       
       animation.clear();
       context.putImageData(imageData, 0, 0);
   }
});
```


Pour chaque pixel, nous rempla√ßons les canaux RGB par leur niveau de gris.

Intuitivement, il serait tentant d'utiliser une moyenne des composantes RGB. Toutefois, puisque notre oeil ne per√ßoit pas toutes les couleurs avec la m√™me sensibilit√©, une telle m√©thode ne serait pas optimale pour l'≈ìil humain. On utilise √† la place la **luminance** du pixel. Ce n'est rien d'autre qu'une combinaison des trois couleurs, mais avec des coefficients pour prendre en compte notre sensibilit√© √† la perception des couleurs.

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale.gif?raw=true)

## Supporter les interactions souris

Une animation c'est bien, mais une animation qui interagit avec la souris, c'est mieux ! Nous allons transformer notre code afin d'appliquer le filtre seulement si notre pointeur se trouve √† l'int√©rieur du canvas.

Nous pouvons d'ores et d√©j√† transformer notre m√©thode de rendu de la sorte :

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        
        // compute isPointerHoverCanvas ...

        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
        
        animation.clear();
        context.putImageData(imageData, 0, 0);
    }
});
```

Plusieurs solutions existent pour d√©terminer si le curseur est, ou n'est pas, √† l'int√©rieur de notre canvas. Selon l'approche, certaines sont plus appropri√©es que d'autres.

### D√©terminer la position de la souris par rapport au canvas

La solution la plus simple consiste √† r√©cup√©rer les coordonn√©es de la souris, les coordonn√©es du canvas, et de v√©rifier si le curseur de la souris est dans la bo√Æte englobante du canvas (ou hitbox).

```js
const pointerCoords = {x: 0, y: 0};

document.addEventListener('pointermove', event => {
    pointerCoords.x = event.clientX;
    pointerCoords.y = event.clientY;
});

const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        // ‚Ä¶

        const boundingBox = canvas.getBoundingClientRect();


        const isPointerHoverCanvas = (
            pointerCoords.x >= boundingBox.left &&
            pointerCoords.y >= boundingBox.top &&
            pointerCoords.x < boundingBox.right &&
            pointerCoords.y < boundingBox.bottom
        );


        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
    }
});
```

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-1.gif?raw=true)

## On corse le jeu !

On va appliquer le filtre grayscale sur toute l'image, et ne faire appara√Ætre les couleurs que sur les pixels autour de notre curseur.

Petite subtilit√© : pour cr√©er un effet plus lisse, on d√©terminera un cercle √† l'int√©rieur duquel les pixels seront color√©s, mais avec une intensit√© inversement proportionnelle √† la distance au centre‚Ä¶

### Rappel G√©om√©trique
Un cercle peut √™tre d√©fini par son centre, et un rayon. Dans notre cas, le centre du cercle correspond tout simplement aux coordonn√©es du curseur. Quand au rayon, nous prendrons une valeur arbitraire.

D√©terminer si un point est dans un cercle revient √† calculer la **collision** entre un point et un cercle.

De nombreux tutoriels existent et expliquent les formules de collision. Pour en savoir plus sur les m√©thodes de collision, jetez un oeil √† ce tutoriel : http://www.jeffreythompson.org/collision-detection/point-circle.php

### Approche g√©n√©rale

Nous allons modifier la m√©thode de rendu afin de v√©rifier, pour chaque pixel de notre imageData s'il est, ou n'est pas, dans le cercle centr√© sur la position de notre curseur. Afin de faciliter le calcul, nous allons nous placer dans le rep√®re g√©om√©trique de notre canvas.

```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    
    const coordsRelativeToCanvas = PointerCoordsHelper.getCoordsRelativeToElement(
        canvas,
        pointerCoords.x,
        pointerCoords.y
    );

    const buffer = imageData.data;
    
    // apply to the whole buffer, execept a circle defined by pointer position
    for (let offset = 0; offset < buffer.length; offset += 4) {
        const pixelOffset = (offset / 4); // pixels have 4 channel in ImageData
        const pixelX = pixelOffset % canvas.width;
        const pixelY = pixelOffset / canvas.width;

        // arbitrary radius
        const radius = 50;

        const isInCircle = CollisionHelper.isPointInCircle(
            pixelX, pixelY,
            coordsRelativeToCanvas.x, coordsRelativeToCanvas.y,
            radius
        );

        const grayscale = rgbToGrayscale(buffer, offset);

        if (isInCircle === false) {
            buffer[offset] = grayscale;
            buffer[offset + 1] = grayscale;
            buffer[offset + 2] = grayscale;
            buffer[offset + 3] = 255;
        } else {
            const distance = GeometryHelper.getDistanceBetween2DPoints(
                pixelX, pixelY,
                coordsRelativeToCanvas.x, coordsRelativeToCanvas.y
            );

            const weight = distance / radius;
            // apply a weight in order to let color intensity increase from the outside to the center
            buffer[offset] = weight * grayscale + (1 - weight) * buffer[offset];
            buffer[offset + 1] = weight * grayscale + (1 - weight) * buffer[offset + 1];
            buffer[offset + 2] = weight * grayscale + (1 - weight) * buffer[offset + 2];
            buffer[offset + 3] = 255;
        }
    }


    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-2.gif?raw=true)

<details>
<summary>‚ö†Ô∏è Remarque sur le calcul de coordonn√©es ( Avanc√©) ‚ö†Ô∏è</summary>

Le lecteur attentif aura remarqu√© qu'on calcule la position du curseur relativement √† notre canvas (c'est-√†-dire telle que l'origine soit le coin sup√©rieur gauche du canvas). 

Ce n'est pas obligatoire, on aurait pu impl√©menter le m√™me filtre en utilisant directement les coordonn√©es du pointeur dans la fen√™tre, mais les √©quations auraient √©t√©s plus compliqu√©es. 

De plus, l'on est parfois amen√© √† travailler avec des canvas dont la r√©solution (*pixel th√©orique*) est diff√©rente de sa taille (*pixel physique*). Dans ce cas, vu que notre algorithme it√®re sur les pixels th√©oriques du canvas `animation.context.width` ou `animation.canvas.width`. Il faudra modifier les √©quations pour prendre en compte ce changement de rep√®re suppl√©mentaire‚Ä¶

</details>

## Sa vision est bas√©e sur le mouvement !


![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-1.gif?raw=true)

"Mouais, c'est pas terrible..."

En effet, on peut mieux faire ! N√©anmoins, concentrons nous sur le positif, nous avons un d√©but de quelque chose ! Nous parvenons √† d√©celer les contours du perroquet lorsque celui-ci entre en mouvement. Le probl√®me c'est que ses d√©placements sont lents, et pas forc√©ment perceptibles d'une frame sur l'autre. De plus, si les mouvements entre deux slices sont minimes, et que nous rafraichissons notre en 60 fps (ce que tentera de faire naturellement notre classe animation), soit une image approximativement toutes les 16ms, et sachant que la persistance r√©tinienne est de l'ordre de 1/25 de secondes (40 ms), il y a de grande chance pour que le mouvement, bien que visible dans notre canvas, ne nous soit pas perceptible.

### Alternative simple

Plut√¥t que de se lancer dans un calcul p√©rilleux sur le taux de rafra√Æchissement optimal, nous allons opter pour une solution b√™te et m√©chante : calculer le mouvement en prenant en compte, non pas la derni√®re frame, mais les N derni√®res frames.

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-2.gif?raw=true)


Voici une tentative se basant sur les 5 derni√®res frames, avec coefficient √©quivalent √† 15. Si les mouvements sont davantage perceptibles, le co√ªt en calcul lui est nettement plus √©lev√©. On passe de 60 fps √† un peu plus de 20 fps. Rien de plus normal, on a presque tripl√© la charge de travail.

Il existe des m√©thodes permettant d'obtenir un r√©sultat plus propre, et moins gourmant en calcul, mais √©galement moins simple √† expliquer. Et vu que ce n'est pas le but premier de cet article, je les passe sous silence. Sachez juste qu'en utilisant des images s√©par√©es dans le temps (r√©gulier, par exemple 40 ms) et une simple moyenne au lieu de frame s√©par√©es par des it√©rations de notre boucle de rendu (potentiellement irr√©guli√®re est trop courte), on se rapproche d'un comportement naturel ;)

## La vie en bleu 

![Alt Text](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/blue-1.gif?raw=true)
